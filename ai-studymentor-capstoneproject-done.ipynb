{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13873972,"sourceType":"datasetVersion","datasetId":8839458}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n#StudyMentor ‚Äì Multi-Agent Exam Helper\n# Kaggle-ready single cell code\n# Requires: GOOGLE_API_KEY in Kaggle Secrets\n\n\n!pip install --upgrade google-generativeai pypdf -q\n\nimport os\nimport google.generativeai as genai\nfrom pypdf import PdfReader\nfrom kaggle_secrets import UserSecretsClient\n\n                          #GEMINI CONFIG \nuser_secrets = UserSecretsClient()\nAPI_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n\nif not API_KEY:\n    raise ValueError(\"‚ùå GOOGLE_API_KEY not found in environment. Set it in Kaggle 'Add-ons ‚Üí Secrets'.\")\n\ngenai.configure(api_key=API_KEY)\nMODEL_NAME = \"gemini-2.5-flash\"\n\n\ndef llm(prompt: str, max_tokens: int = 300) -> str:\n    model = genai.GenerativeModel(MODEL_NAME)  #  use global model name\n    resp = model.generate_content(prompt)\n    try:\n        return resp.text\n    except:\n        return \"‚ö† Model refused to respond due to safety or access limitations.\"\n\n                     #GLOBAL STATE \npdf_text = None          # full text from loaded PDF\npdf_path_global = None   # last loaded pdf path\n\n\n                      # UTILITY FUNCTIONS \ndef read_pdf(path: str) -> str:\n    \"\"\"Read all text from a PDF file.\"\"\"\n    reader = PdfReader(path)\n    pages = [page.extract_text() or \"\" for page in reader.pages]\n    return \"\\n\".join(pages)\n\n\ndef chunk_text(text: str, max_chars: int = 8000):\n    \"\"\"Split long text into chunks of max_chars.\"\"\"\n    chunks = []\n    start = 0\n    n = len(text)\n    while start < n:\n        end = min(start + max_chars, n)\n        chunks.append(text[start:end])\n        start = end\n    return chunks\n\n\n                      # AGENT 1: PDF READER \ndef agent_pdf_reader():\n    global pdf_text, pdf_path_global\n    print(\"\\n--- PDF READER AGENT ---\")\n    print(\"Example path (Kaggle): /kaggle/input/your-folder/your-file.pdf\")\n    path = input(\"Enter PDF file path: \").strip()\n    if not path:\n        print(\"‚ùå No path provided.\")\n        return\n\n    try:\n        text = read_pdf(path)\n        if not text.strip():\n            print(\"‚ö† PDF loaded but no text extracted (maybe scanned/non-text PDF).\")\n        else:\n            pdf_text = text\n            pdf_path_global = path\n            print(f\"‚úÖ PDF loaded successfully from: {path}\")\n            print(f\"Approx length: {len(pdf_text)} characters\")\n    except Exception as e:\n        print(\"‚ùå Error reading PDF:\", e)\n\n\n                            # AGENT 2: PDF SUMMARIZER \ndef agent_pdf_summarizer():\n    global pdf_text, pdf_path_global\n    print(\"\\n--- PDF SUMMARIZER AGENT ---\")\n\n    if not pdf_text or len(pdf_text.strip()) < 200:\n        print(\"‚ùå PDF text is too short or unreadable.\")\n        return\n\n    mode = input(\"Summary type? (short/medium/long) [default: medium]: \").strip().lower() or \"medium\"\n    if mode == \"short\":\n        chunks_limit = 1\n        max_tokens = 350\n    elif mode == \"long\":\n        chunks_limit = 5\n        max_tokens = 900\n    else:\n        chunks_limit = 2\n        max_tokens = 550\n\n    chunks = chunk_text(pdf_text, max_chars=4500)  # smaller chunks prevent safety block\n    chunks = chunks[:chunks_limit]\n\n    partial_summaries = []\n\n    for i, ch in enumerate(chunks, 1):\n        print(f\"Processing chunk {i}/{len(chunks)} ...\")\n\n        if len(ch.strip()) < 50:\n            print(f\"‚ö† Chunk {i} is too small, skipping.\")\n            continue\n\n        prompt = f\"\"\"\nSummarize the following academic text into bullet points and key concepts.\nFocus on clarity, definitions, formulas, and important reasoning.\nAvoid long paragraphs.\n\nCONTENT:\n{ch}\n\"\"\"\n\n        try:\n            summary = llm(prompt, max_tokens=max_tokens)\n\n            if not summary or summary.strip() == \"\":\n                print(f\"‚ö† Chunk {i} returned empty. Skipping.\")\n                continue\n\n            partial_summaries.append(summary)\n            print(f\"‚úÖ Chunk {i} summarized\")\n\n        except Exception as e:\n            print(f\"‚ùå Error summarizing chunk {i}: {e}\")\n            continue\n\n    if not partial_summaries:\n        print(\"‚ùå No usable summaries were produced. Try summarizing manually or reduce chunk size.\")\n        return\n\n    separator = \"\\n\\n---\\n\\n\".join(partial_summaries)\n\n    final_prompt = f\"\"\"\nCombine the following short summaries into a well-structured final set of study notes.\nUse headings, bullet points, examples, and keep everything short and exam-focused.\n\nTEXT:\n{separator}\n\"\"\"\n\n    try:\n        final_summary = llm(final_prompt, max_tokens=900)\n        print(\"\\n====== FINAL SHORT NOTES ======\\n\")\n        print(final_summary)\n        print(\"\\n==============================\\n\")\n    except Exception as e:\n        print(\"‚ùå Failed final merge:\", e)\n\n                               # AGENT 3: MCQ GENERATOR \ndef agent_mcq_generator():\n    global pdf_text, pdf_path_global\n\n    print(\"\\n--- MCQ GENERATOR AGENT ---\")\n\n    if not pdf_text or len(pdf_text.strip()) < 100:\n        print(\"‚ùå PDF text is empty or too short. Try another PDF or check if it is scanned.\")\n        return\n\n    try:\n        num_q = int(input(\"How many MCQs do you want? (e.g. 10): \").strip())\n    except:\n        num_q = 10\n\n    # Reduce context to avoid filter issue\n    base_text = pdf_text[:8000]\n\n    prompt = f\"\"\"\nYou are an exam MCQ creator.\n\nGenerate {num_q} high-quality MCQs from the study material below.\n\nRules:\n- Each question must have options A, B, C, D\n- Clearly state: Answer: <option>\n- No overly personal or unsafe content\n- Focus on academic & safe concepts only\n\nCONTENT:\n{base_text}\n\"\"\"\n\n    try:\n        response = llm(prompt, max_tokens=1000)\n\n        if not response or len(response.strip()) == 0:\n            print(\"‚ö† Model returned empty result. Trying again with simpler prompt...\")\n            response = llm(\"Create \" + str(num_q) + \" academic multiple choice questions with answers.\")\n\n        print(\"\\n====== GENERATED MCQs ======\\n\")\n        print(response)\n        print(\"\\n============================\\n\")\n\n    except Exception as e:\n        print(\"‚ùå Model failed. Try summarizing first or reducing MCQ count.\")\n        print(\"Error:\", e)\n\n                                      # AGENT 4: STUDY PLAN GENERATOR \ndef agent_study_plan():\n    print(\"\\n--- STUDY PLAN AGENT ---\")\n    exam_name = input(\"Exam name (e.g. DAA Sessional, Python Viva, etc.): \").strip() or \"Your Exam\"\n    days_str = input(\"In how many days is your exam? (e.g. 7): \").strip()\n    try:\n        days_left = int(days_str)\n    except:\n        days_left = 7\n\n    topics = input(\"List key topics (comma separated):\\n(e.g. Arrays, Linked List, Trees, Graphs):\\n\").strip()\n    weak = input(\"Which topics are you weak in? (comma separated, or leave blank): \").strip()\n    daily_hours_str = input(\"How many hours per day can you study? (e.g. 3): \").strip()\n    try:\n        daily_hours = float(daily_hours_str)\n    except:\n        daily_hours = 3.0\n\n    prompt = f\"\"\"\nYou are an AI Study Mentor.\n\nCreate a detailed {days_left}-day study plan for the exam: {exam_name}.\n\nInformation:\n- Topics: {topics or \"not specified\"}\n- Weak topics: {weak or \"not specified\"}\n- Hours per day: {daily_hours}\n\nPlan requirements:\n- Day-wise schedule with time blocks\n- More focus/time on weak topics\n- Include revision days\n- Include MCQ practice / previous year questions\n- Use a clean, bullet/markdown format\n- Very practical and realistic\n\"\"\"\n\n    try:\n        plan = llm(prompt, max_tokens=1200)\n        print(\"\\n====== STUDY PLAN ======\\n\")\n        print(plan)\n        print(\"\\n========================\\n\")\n    except Exception as e:\n        print(\"‚ùå Error generating study plan:\", e)\n\n\n                         # AGENT 5: DIAGRAM GENERATOR\ndef agent_diagram_generator():\n    print(\"\\n--- DIAGRAM GENERATOR AGENT ---\")\n    concept = input(\"Enter the concept for which you want a diagram (e.g. QuickSort, OS layers, CNN, Tree traversal): \").strip()\n    if not concept:\n        print(\"‚ùå No concept provided.\")\n        return\n\n    style = input(\"Diagram style? (ascii/markdown/simple) [default: ascii]: \").strip().lower() or \"ascii\"\n\n    prompt = f\"\"\"\nYou are a teaching assistant.\n\nCreate a clear {style} diagram to explain this concept to a student:\nCONCEPT: {concept}\n\nRequirements:\n- Use only text (no images).\n- Use boxes, arrows, indentation, or tree-style layouts.\n- After the diagram, add 3‚Äì5 bullet points explaining the key idea.\n\"\"\"\n\n    try:\n        diagram = llm(prompt, max_tokens=700)\n        print(\"\\n====== GENERATED DIAGRAM ======\\n\")\n        print(diagram)\n        print(\"\\n================================\\n\")\n    except Exception as e:\n        print(\"‚ùå Error generating diagram:\", e)\n\n\n                           #ORCHESTRATOR / MAIN MENU \ndef main_menu():\n    print(\"====================================\")\n    print(\"    AI StudyMentor ‚Äì Multi-Agent    \")\n    print(\"====================================\")\n\n    while True:\n        print(\"\\nWhat do you want to do?\")\n        print(\"1. Load / Read a PDF\")\n        print(\"2. Summarize loaded PDF into short notes\")\n        print(\"3. Generate MCQs from loaded PDF\")\n        print(\"4. Create a personalized study plan\")\n        print(\"5. Generate a text-based diagram for a concept\")\n        print(\"0. Exit\")\n\n        choice = input(\"Enter your choice (0-5): \").strip()\n\n        if choice == \"1\":\n            agent_pdf_reader()\n        elif choice == \"2\":\n            agent_pdf_summarizer()\n        elif choice == \"3\":\n            agent_mcq_generator()\n        elif choice == \"4\":\n            agent_study_plan()\n        elif choice == \"5\":\n            agent_diagram_generator()\n        elif choice == \"0\":\n            print(\"üëã Exiting AI StudyMentor. All the best for your exam!\")\n            break\n        else:\n            print(\"‚ùå Invalid choice. Please select a number between 0 and 5.\")\n\n\n# ------------- RUN PROGRAM \nif __name__ == \"__main__\":\n    main_menu()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:04:39.912056Z","iopub.execute_input":"2025-11-30T06:04:39.912621Z","iopub.status.idle":"2025-11-30T06:22:59.189456Z","shell.execute_reply.started":"2025-11-30T06:04:39.912584Z","shell.execute_reply":"2025-11-30T06:22:59.188569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfor root, dirs, files in os.walk(\"/kaggle/input\"):\n    for file in files:\n        print(os.path.join(root, file))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T05:48:03.561730Z","iopub.execute_input":"2025-11-30T05:48:03.562110Z","iopub.status.idle":"2025-11-30T05:48:03.571289Z","shell.execute_reply.started":"2025-11-30T05:48:03.562076Z","shell.execute_reply":"2025-11-30T05:48:03.570336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = genai.GenerativeModel(\"gemini-2.5-flash\")\nresp = model.generate_content(\"Hello!\")\nprint(resp.text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T06:42:22.359374Z","iopub.execute_input":"2025-11-26T06:42:22.360210Z","iopub.status.idle":"2025-11-26T06:42:24.989947Z","shell.execute_reply.started":"2025-11-26T06:42:22.360174Z","shell.execute_reply":"2025-11-26T06:42:24.989064Z"}},"outputs":[],"execution_count":null}]}